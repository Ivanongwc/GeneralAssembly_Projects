{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/Users/ivanong/GA_Ivan/week-06/labs/python-webscraping_opentable-lab-master/chromedriver/chromedriver')\n",
    "driver.get('https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page=0')\n",
    "sleep(2)\n",
    "html=driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "df = pd.DataFrame({'links':links})\n",
    "\n",
    "# Loop through pages to extract links to job descriptions\n",
    "for pageNum in range(0,200):\n",
    "    driver.get(\"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\".format(pageNum))\n",
    "    sleep(randint(5,10))\n",
    "    html = driver.page_source\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "    url = \"https://www.mycareersfuture.sg\"\n",
    "    \n",
    "# Extract links to job posts\n",
    "    for link in html.find_all('a', {\"class\":\"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"}):\n",
    "        try:\n",
    "            links.append(url+link.get('href'))  \n",
    "        except: \n",
    "            links.append(np.nan)  \n",
    "\n",
    "# Dataframe for storage and export links to CSV file\n",
    "df = pd.DataFrame({'links':links}, index=None).to_csv(\"links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = pd.read_csv(\"links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store the data\n",
    "\n",
    "Company = []\n",
    "Job_title = []\n",
    "Location = []\n",
    "Employment_type = []\n",
    "Seniority = []\n",
    "Job_category = []\n",
    "Salary_range = []\n",
    "Salary_type = []\n",
    "Num_of_applications = []\n",
    "Last_posted_date = []\n",
    "Expiry = []\n",
    "Responsibilities = []\n",
    "Requirements = []\n",
    "Company_info = []\n",
    "\n",
    "# Extract link to job descriptions\n",
    "driver = webdriver.Chrome('/Users/ivanong/GA_Ivan/week-06/labs/python-webscraping_opentable-lab-master/chromedriver/chromedriver')\n",
    "for i in range (len(url)):\n",
    "    try: \n",
    "        driver.get(url.links[i])\n",
    "        sleep(3)\n",
    "        html = driver.page_source\n",
    "        # Parsing via Beautiful Soup\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        job_info = html.find(\"body\", {\"class\": \"overflow-x-hidden\"})\n",
    "        try: \n",
    "            j1 = job_info.find('p', {\"name\": \"company\"}).text  \n",
    "        except: \n",
    "            j1 = np.nan         \n",
    "        company.append(j1)\n",
    "        try: \n",
    "            j2 = job_info.find('h1', {\"id\":\"job_title\"}).text    \n",
    "        except: \n",
    "            j2 = np.nan    \n",
    "        job_title.append(j2)\n",
    "        try: \n",
    "            j3 = job_info.find('p',{'id':\"address\"}).text    \n",
    "        except:\n",
    "            j3 = np.nan    \n",
    "        location.append(j3)\n",
    "        try:\n",
    "            j4 = job_info.find('p',{'id':\"employment_type\"}).text    \n",
    "        except: \n",
    "            j4 = np.nan\n",
    "        employment_type.append(j4)\n",
    "        try: \n",
    "            j5 = job_info.find('p',{'id':\"seniority\"}).text\n",
    "        except:\n",
    "            j5 = np.nan\n",
    "        seniority.append(j5)\n",
    "        try: \n",
    "            j6 = job_info.find('p',{'id':\"job-categories\"}).text\n",
    "        except: \n",
    "            j6 = np.nan\n",
    "        job_category.append(j6)\n",
    "        try:\n",
    "            j7 = job_info.find('span',{\"class\":\"salary_range\"}).text\n",
    "        except:\n",
    "            j7 = np.nan\n",
    "        salary_range.append(j7)\n",
    "        try: \n",
    "            j8 = job_info.find('span',{\"class\":\"salary_type\"}).text\n",
    "        except:\n",
    "            j8 = np.nan\n",
    "        salary_type.append(j8)\n",
    "        try: \n",
    "            j9 = job_info.find('span',{\"id\":\"num_of_applications\"}).text\n",
    "        except:\n",
    "            j9 = np.nan\n",
    "        num_of_applications.append(j9)\n",
    "        try: \n",
    "            j10 = job_info.find('span',{\"id\":\"last_posted_date\"}).text\n",
    "        except: \n",
    "            j10 = np.nan\n",
    "        last_posted_date.append(j10)\n",
    "        try: \n",
    "            j11 = job_info.find('span',{\"id\":\"expiry_date\"}).text\n",
    "        except: \n",
    "            j11 = np.nan\n",
    "        expiry.append(j11)\n",
    "        try:\n",
    "            j12 = job_info.find('div',{\"id\":\"description-content\"}).text\n",
    "        except:\n",
    "            j12 = np.nan\n",
    "        responsibilities.append(j12)\n",
    "        try: \n",
    "            j13 = job_info.find('div',{\"id\":\"requirements-content\"}).text\n",
    "        except:\n",
    "            j13  = np.nan\n",
    "        requirements.append(j13)\n",
    "        \n",
    "#   NaN values if info is not captured\n",
    "    except:\n",
    "        company.append(np.nan)\n",
    "        job_title.append(np.nan)\n",
    "        location.append(np.nan)\n",
    "        employment_type.append(np.nan)\n",
    "        seniority.append(np.nan)\n",
    "        job_category.append(np.nan)\n",
    "        salary_range.append(np.nan)\n",
    "        salary_type.append(np.nan)\n",
    "        num_of_applications.append(np.nan)\n",
    "        last_posted_date.append(np.nan)\n",
    "        expiry.append(np.nan)\n",
    "        responsibilities.append(np.nan)\n",
    "        requirements.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "                    'company':company, 'job_title': job_title, 'location': location, \n",
    "                    'employment_type':employment_type, 'seniority': seniority, \"job_category\": job_category, \n",
    "                   \"salary_range\":salary_range, \"salary_type\":salary_type, \n",
    "                    \"num_of_applications\": num_of_applications, \"last_posted_date\": last_posted_date,\n",
    "                    \"expiry\": expiry, \"responsibilities\":responsibilities, \"requirements\": requirements, \n",
    "                   }, index=None).to_csv(\"jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('jobs_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3834, 13)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing null rows numbers that were not captured into a list\n",
    "\n",
    "null_rows = df[df['company'].isnull()].copy()\n",
    "nulls_list = [i for i in null_rows['Unnamed: 0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nulls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the code to fill null rows\n",
    "company = []\n",
    "job_title = []\n",
    "location = []\n",
    "employment_type = []\n",
    "seniority = []\n",
    "job_category = []\n",
    "salary_range = []\n",
    "salary_type = []\n",
    "num_of_applications = []\n",
    "last_posted_date = []\n",
    "expiry = []\n",
    "responsibilities = []\n",
    "requirements = []\n",
    "company_info = []\n",
    "\n",
    "# Running the same for loop to run through null rows\n",
    "for i in nulls_list:\n",
    "    try: \n",
    "        driver.get(url.links[i])\n",
    "        sleep(6)\n",
    "        html = driver.page_source\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        job_info = html.find(\"body\", {\"class\": \"overflow-x-hidden\"})\n",
    "        try: \n",
    "            j1 = job_info.find('p', {\"name\": \"company\"}).text  \n",
    "        except: \n",
    "            j1 = np.nan         \n",
    "        company.append(j1)\n",
    "        try: \n",
    "            j2 = job_info.find('h1', {\"id\":\"job_title\"}).text    \n",
    "        except: \n",
    "            j2 = np.nan    \n",
    "        job_title.append(j2)\n",
    "        try: \n",
    "            j3 = job_info.find('p',{'id':\"address\"}).text    \n",
    "        except:\n",
    "            j3 = np.nan    \n",
    "        location.append(j3)\n",
    "        try:\n",
    "            j4 = job_info.find('p',{'id':\"employment_type\"}).text    \n",
    "        except: \n",
    "            j4 = np.nan\n",
    "        employment_type.append(j4)\n",
    "        try: \n",
    "            j5 = job_info.find('p',{'id':\"seniority\"}).text\n",
    "        except:\n",
    "            j5 = np.nan\n",
    "        seniority.append(j5)\n",
    "        try: \n",
    "            j6 = job_info.find('p',{'id':\"job-categories\"}).text\n",
    "        except: \n",
    "            j6 = np.nan\n",
    "        job_category.append(j6)\n",
    "        try:\n",
    "            j7 = job_info.find('span',{\"class\":\"salary_range\"}).text\n",
    "        except:\n",
    "            j7 = np.nan\n",
    "        salary_range.append(j7)\n",
    "        try: \n",
    "            j8 = job_info.find('span',{\"class\":\"salary_type\"}).text\n",
    "        except:\n",
    "            j8 = np.nan\n",
    "        salary_type.append(j8)\n",
    "        try: \n",
    "            j9 = job_info.find('span',{\"id\":\"num_of_applications\"}).text\n",
    "        except:\n",
    "            j9 = np.nan\n",
    "        num_of_applications.append(j9)\n",
    "        try: \n",
    "            j10 = job_info.find('span',{\"id\":\"last_posted_date\"}).text\n",
    "        except: \n",
    "            j10 = np.nan\n",
    "        last_posted_date.append(j10)\n",
    "        try: \n",
    "            j11 = job_info.find('span',{\"id\":\"expiry_date\"}).text\n",
    "        except: \n",
    "            j11 = np.nan\n",
    "        expiry.append(j11)\n",
    "        try:\n",
    "            j12 = job_info.find('div',{\"id\":\"description-content\"}).text\n",
    "        except:\n",
    "            j12 = np.nan\n",
    "        responsibilities.append(j12)\n",
    "        try: \n",
    "            j13 = job_info.find('div',{\"id\":\"requirements-content\"}).text\n",
    "        except:\n",
    "            j13  = np.nan\n",
    "        requirements.append(j13)\n",
    "\n",
    "    except:\n",
    "        company.append(np.nan)\n",
    "        job_title.append(np.nan)\n",
    "        location.append(np.nan)\n",
    "        employment_type.append(np.nan)\n",
    "        seniority.append(np.nan)\n",
    "        job_category.append(np.nan)\n",
    "        salary_range.append(np.nan)\n",
    "        salary_type.append(np.nan)\n",
    "        num_of_applications.append(np.nan)\n",
    "        last_posted_date.append(np.nan)\n",
    "        expiry.append(np.nan)\n",
    "        responsibilities.append(np.nan)\n",
    "        requirements.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = pd.DataFrame({\n",
    "                    'company':company, 'job_title': job_title, 'location': location, \n",
    "                    'employment_type':employment_type, 'seniority': seniority, \"job_category\": job_category, \n",
    "                   \"salary_range\":salary_range, \"salary_type\":salary_type, \n",
    "                    \"num_of_applications\": num_of_applications, \"last_posted_date\": last_posted_date,\n",
    "                    \"expiry\": expiry, \"responsibilities\":responsibilities, \"requirements\": requirements, \n",
    "                   }, index=None).to_csv(\"jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 13)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df1,df_null])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 13)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all rows are full\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting dataframe to csv\n",
    "df_total.to_csv(\"jobs_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
