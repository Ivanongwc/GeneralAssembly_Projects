{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/Users/ivanong/GA_Ivan/week-06/labs/python-webscraping_opentable-lab-master/chromedriver/chromedriver')\n",
    "driver.get('https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page=0')\n",
    "sleep(2)\n",
    "html=driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e97ae3380e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpageNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "links = []\n",
    " \n",
    "html_page = urlopen(\"http://mycareersfuture.sg\")\n",
    "soup = BeautifulSoup(html_page)\n",
    "\n",
    "for pageNum in range(0,100):\n",
    "    driver.get(\"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\".format(pageNum))\n",
    "    sleep(randint(5,10))\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "    url = \"https://www.mycareersfuture.sg\"\n",
    "    \n",
    "    for link in soup.findAll('a', {\"class\":\"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"}):\n",
    "        try:\n",
    "            links.append(url+link.get('href'))  \n",
    "        except: \n",
    "            links.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver called \"driver.\"\n",
    "driver = webdriver.Chrome('/Users/ivanong/GA_Ivan/week-06/labs/python-webscraping_opentable-lab-master/chromedriver/chromedriver')\n",
    "links = []\n",
    "df = pd.DataFrame({'links':links})\n",
    "\n",
    "# Loop through pages to extract links to job descriptions\n",
    "for pageNum in range(0,200):\n",
    "    driver.get(\"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\".format(pageNum))\n",
    "    sleep(randint(5,10))\n",
    "# Grab the page source.\n",
    "    html = driver.page_source\n",
    "# Beautiful Soup it!\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "    url = \"https://www.mycareersfuture.sg\"\n",
    "    \n",
    "# Extract links to job posts\n",
    "    for link in html.find_all('a', {\"class\":\"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"}):\n",
    "        try:\n",
    "            links.append(url+link.get('href'))  \n",
    "        except: \n",
    "            links.append(np.nan)  \n",
    "\n",
    "# Create a DF to store the data and export links to CSV file\n",
    "df = pd.DataFrame({'links':links}, index=None).to_csv(\"links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = pd.read_csv(\"links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9634da7a23e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0;34m\"num_of_applications\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_of_applications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last_posted_date\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlast_posted_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                         \u001b[0;34m\"expiry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpiry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"responsibilities\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresponsibilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"requirements\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                        }, index=None).to_csv(\"jobs.csv\")\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         libwriters.write_csv_rows(self.data, ix, self.nlevels,\n\u001b[0;32m--> 313\u001b[0;31m                                   self.cols, self.writer)\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create lists for jobs data\n",
    "company = []\n",
    "job_title = []\n",
    "location = []\n",
    "employment_type = []\n",
    "seniority = []\n",
    "job_category = []\n",
    "salary_range = []\n",
    "salary_type = []\n",
    "num_of_applications = []\n",
    "last_posted_date = []\n",
    "expiry = []\n",
    "responsibilities = []\n",
    "requirements = []\n",
    "company_info = []\n",
    "\n",
    "# Loop through pages to extract links to job descriptions\n",
    "driver = webdriver.Chrome('/Users/ivanong/GA_Ivan/week-06/labs/python-webscraping_opentable-lab-master/chromedriver/chromedriver')\n",
    "for i in range (len(url)):\n",
    "    try: \n",
    "        driver.get(url.links[i])\n",
    "    # Allow page to load   \n",
    "        sleep(3)\n",
    "    # Grab the page source.\n",
    "        html = driver.page_source\n",
    "    # Beautiful Soup it!\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # Extract section of HTML that contains job information we are after\n",
    "        job_info = html.find(\"body\", {\"class\": \"overflow-x-hidden\"})\n",
    "    # Extract job information \n",
    "        try: \n",
    "            j1 = job_info.find('p', {\"name\": \"company\"}).text  \n",
    "        except: \n",
    "            j1 = np.nan         \n",
    "        company.append(j1)\n",
    "        try: \n",
    "            j2 = job_info.find('h1', {\"id\":\"job_title\"}).text    \n",
    "        except: \n",
    "            j2 = np.nan    \n",
    "        job_title.append(j2)\n",
    "        try: \n",
    "            j3 = job_info.find('p',{'id':\"address\"}).text    \n",
    "        except:\n",
    "            j3 = np.nan    \n",
    "        location.append(j3)\n",
    "        try:\n",
    "            j4 = job_info.find('p',{'id':\"employment_type\"}).text    \n",
    "        except: \n",
    "            j4 = np.nan\n",
    "        employment_type.append(j4)\n",
    "        try: \n",
    "            j5 = job_info.find('p',{'id':\"seniority\"}).text\n",
    "        except:\n",
    "            j5 = np.nan\n",
    "        seniority.append(j5)\n",
    "        try: \n",
    "            j6 = job_info.find('p',{'id':\"job-categories\"}).text\n",
    "        except: \n",
    "            j6 = np.nan\n",
    "        job_category.append(j6)\n",
    "        try:\n",
    "            j7 = job_info.find('span',{\"class\":\"salary_range\"}).text\n",
    "        except:\n",
    "            j7 = np.nan\n",
    "        salary_range.append(j7)\n",
    "        try: \n",
    "            j8 = job_info.find('span',{\"class\":\"salary_type\"}).text\n",
    "        except:\n",
    "            j8 = np.nan\n",
    "        salary_type.append(j8)\n",
    "        try: \n",
    "            j9 = job_info.find('span',{\"id\":\"num_of_applications\"}).text\n",
    "        except:\n",
    "            j9 = np.nan\n",
    "        num_of_applications.append(j9)\n",
    "        try: \n",
    "            j10 = job_info.find('span',{\"id\":\"last_posted_date\"}).text\n",
    "        except: \n",
    "            j10 = np.nan\n",
    "        last_posted_date.append(j10)\n",
    "        try: \n",
    "            j11 = job_info.find('span',{\"id\":\"expiry_date\"}).text\n",
    "        except: \n",
    "            j11 = np.nan\n",
    "        expiry.append(j11)\n",
    "        try:\n",
    "            j12 = job_info.find('div',{\"id\":\"description-content\"}).text\n",
    "        except:\n",
    "            j12 = np.nan\n",
    "        responsibilities.append(j12)\n",
    "        try: \n",
    "            j13 = job_info.find('div',{\"id\":\"requirements-content\"}).text\n",
    "        except:\n",
    "            j13  = np.nan\n",
    "        requirements.append(j13)\n",
    "#   Input NaN values for broken links\n",
    "    except:\n",
    "        company.append(np.nan)\n",
    "        job_title.append(np.nan)\n",
    "        location.append(np.nan)\n",
    "        employment_type.append(np.nan)\n",
    "        seniority.append(np.nan)\n",
    "        job_category.append(np.nan)\n",
    "        salary_range.append(np.nan)\n",
    "        salary_type.append(np.nan)\n",
    "        num_of_applications.append(np.nan)\n",
    "        last_posted_date.append(np.nan)\n",
    "        expiry.append(np.nan)\n",
    "        responsibilities.append(np.nan)\n",
    "        requirements.append(np.nan)\n",
    "\n",
    "\n",
    "    df1 = pd.DataFrame({\n",
    "                        'company':company, 'job_title': job_title, 'location': location, \n",
    "                        'employment_type':employment_type, 'seniority': seniority, \"job_category\": job_category, \n",
    "                       \"salary_range\":salary_range, \"salary_type\":salary_type, \n",
    "                        \"num_of_applications\": num_of_applications, \"last_posted_date\": last_posted_date,\n",
    "                        \"expiry\": expiry, \"responsibilities\":responsibilities, \"requirements\": requirements, \n",
    "                       }, index=None).to_csv(\"jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
